{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "EPSILON_START = 1\n",
    "EPSILON_END = .005\n",
    "EPSILON_DECAY = .995\n",
    "LR = 5e-4\n",
    "UPDATE_EVERY = 4\n",
    "REPLACE_EVERY = 500\n",
    "TAU = .001\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "class DQN():\n",
    "    def __init__(self, env, eps):\n",
    "        # Tell the agent what game space we're playing in\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.env = env\n",
    "                \n",
    "        # Create any parameters we'll be adjusting internally\n",
    "        self.epsilon = eps\n",
    "        \n",
    "        # Creating the neural networks\n",
    "        self.qnetwork_expected = self.create_model()\n",
    "        self.qnetwork_target = self.create_model()\n",
    "        \n",
    "        # Creating the memory for experience replay, and a time step\n",
    "        self.memory = deque(maxlen=BUFFER_SIZE)\n",
    "        self.t_step = 0\n",
    "        self.t_step_2 = 0\n",
    "        self.episodes = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        model   = Sequential()\n",
    "        model.add(Dense(64, activation=\"relu\", input_shape=(8,)))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=LR))\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Take a random action if we roll a number less than epsilon, else do what our train network says to do\n",
    "        threshold = random.random()\n",
    "        if threshold < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        if threshold >= self.epsilon:\n",
    "            action = np.argmax(self.qnetwork_expected.predict(state)[0])\n",
    "\n",
    "#         # Decay epsilon after each action\n",
    "#         if self.epsilon > EPSILON_END:\n",
    "#             self.epsilon *= EPSILON_DECAY\n",
    "#             self.epsilon = max(EPSILON_END, self.epsilon)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Set how often we'll update our train network weights\n",
    "        self.t_step += 1 \n",
    "        if self.t_step % UPDATE_EVERY == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                self.update()\n",
    "\n",
    "        # Set how often we'll update our target network with train weights\n",
    "        self.t_step_2 += 1\n",
    "        if self.t_step_2 % UPDATE_EVERY ==0:\n",
    "            self.train_target()\n",
    "\n",
    "        # Tell it to pop if the memory is at buffer size\n",
    "        if len(self.memory) > BUFFER_SIZE:\n",
    "            self.memory.popleft()\n",
    "    \n",
    "    def update(self):\n",
    "        samples = random.sample(self.memory, BATCH_SIZE)\n",
    "        for sample in samples:\n",
    "            state, action, reward, next_state, done = sample\n",
    "            target = self.qnetwork_target.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.qnetwork_target.predict(next_state)[0])\n",
    "                target[0][action] = reward + Q_future * GAMMA\n",
    "            self.qnetwork_expected.fit(state, target, epochs=2, verbose=0)\n",
    "\n",
    "    def train_target(self):\n",
    "        weights = self.qnetwork_expected.get_weights()\n",
    "        target_weights = self.qnetwork_target.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * TAU + target_weights[i] * (1 - TAU)\n",
    "        self.qnetwork_target.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self):\n",
    "        qnetwork_expected.save('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    trials  = 1000\n",
    "    trial_len = 500\n",
    "    eps_decay = 0.995\n",
    "    epsilon = 1\n",
    "    dqn_agent = DQN(env=env, eps = epsilon)\n",
    "    \n",
    "    def play_one(env, epsilon):\n",
    "        dqn_agent = DQN(env=env, eps = epsilon)\n",
    "        state = env.reset()\n",
    "        print(state)\n",
    "        state = env.reset().reshape(1,8)\n",
    "        done = False\n",
    "        full_reward_received = False\n",
    "        totalreward = 0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = dqn_agent.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            new_state = new_state.reshape(1,8)\n",
    "            dqn_agent.step(state, action, reward, new_state, done)\n",
    "            totalreward += reward\n",
    "            steps += 1\n",
    "        return totalreward, steps\n",
    "    \n",
    "    n = 10000\n",
    "    totalrewards = np.empty(n)\n",
    "    for i in range (n):\n",
    "        totalreward, steps = play_one(env=env, epsilon = epsilon)\n",
    "        totalrewards[i] = totalreward\n",
    "        epsilon *= eps_decay\n",
    "        if i % 5 == 0:\n",
    "            print(\"episode:\", i, \"total reward:\", totalreward, \"epsilon:\", epsilon, \"avg reward (last 50):\", totalrewards[max(0, i-50):(i+1)].mean())\n",
    "        if totalrewards[max(0, n-100):(n+1)].mean() >= 200:\n",
    "            dqn_agent.save_model \n",
    "            break\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DQN.save_model of <__main__.DQN object at 0x1a5e4e4128>>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 1\n",
    "dqn_agent = DQN(env=env, eps = epsilon)\n",
    "dqn_agent.save_model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
